---
title: "Machine Learning Course Project Write-Up"
author: "Alok Pattani"
date: "Sunday, February 22, 2015"
output: html_document
---
  
**LOADING EXTERNAL PACKAGES USED IN THIS WORK**
```{r, results ='hide', message = FALSE, collapse =TRUE, strip.white = TRUE}
library(lattice)
library(ggplot2)
library(caret)
library(rpart)
library(MASS)
library(grid)
library(mvtnorm)
library(stats4)
library(modeltools)
library(zoo)
library(sandwich)
library(strucchange)
library(randomForest)
library(combinat)
library(klaR)
```

**READ IN AND LOOK AT PROVIDED TRAINING AND TESTING DATA SETS**
```{r, results = 'hide', message = FALSE, collapse = TRUE, strip.white = TRUE}
pmltraining <- read.csv("pml-training.csv")
pmltesting <- read.csv("pml-testing.csv")

summary(pmltraining)
summary(pmltesting)
```

**REMOVING INCOMPLETE OR MISSING DATA FROM INITIAL TRAINING SET**
```{r, results = 'hide', message = FALSE, collapse = TRUE, strip.white = TRUE}
#Identify columns with NAs or blanks, to be eliminated later
colsToKeep <- (colSums(is.na(pmltraining)) == 0 & colSums(pmltraining == "") == 0)
#First 7 columns aren't really germane to prediction, mark for elimination as well
colsToKeep[1:7] <- FALSE

#Subset down to meaningful columns and eliminate "no window" rows (look different from rest of data)
usabledata <- subset(pmltraining, new_window == "no", select = colsToKeep)

head(usabledata)
dim(usabledata)
summary(usabledata)
```

**TAKE USABLE DATA FROM PROVIDED TRAINING SET, SPLIT INTO "NEW" TRAINING AND TEST SETS**  
The model can be chosen based on analysis done on the "new" training set. After that, we can use this "new" test set for cross-validation and out-of-sample error estimation, before applying the model to the "final" test set of 20 observations that are to be submitted.
```{r, results = 'hide', message = FALSE, warning = FALSE, collapse = TRUE, strip.white = TRUE}
set.seed(23)
inTrain <- createDataPartition(y = usabledata$classe, p = 0.75, list = FALSE)

training <- usabledata[inTrain, ]
testing <- usabledata[-inTrain, ]

#Creating training data set without response for use in some models
training_noresponse <- subset(training, select = -c(classe))
```
  
For the next few steps, various techniques learned in the course are used to try to predict the "classe" variable on the training data set. In each case, we use the confusion matrix to look at the accuracy of the predictions on the training data.  
  
**BUILD PREDICTION USING CLASSIFICATION TREE ON TRAINING DATA**
```{r, message = FALSE, warning = FALSE, collapse = TRUE, strip.white = TRUE}
rpart_model <- rpart(classe ~ ., data = training)

confusionMatrix(predict(rpart_model, type = "class"), training$classe)
```

**BUILD PREDICTION USING BAGGING ON TRAINING DATA**
```{r, message = FALSE, warning = FALSE,  collapse = TRUE, strip.white = TRUE}
bag_model <- bag(y = training$classe, x = training_noresponse, 
    bagControl = bagControl(fit = ctreeBag$fit, predict = ctreeBag$pred, 
    aggregate = ctreeBag$aggregate))  

confusionMatrix(predict(bag_model, newdata = training), training$classe)
```

**BUILD PREDICTION USING RANDOM FOREST ON TRAINING DATA**  
```{r, message = FALSE, warning = FALSE, collapse = TRUE, strip.white = TRUE}
ranfor_model <- randomForest(classe ~ ., data = training)

ranfor_model
```
This random forest implementation already does cross validation, providing the out of sample error estimate as part of the modeling process.  In this case, that value is `r paste0(round(ranfor_model$err.rate[500,1]*100, 2), "%")`.
  
**BUILD PREDICTION USING LINEAR DISCRIMINANT ANALYSIS ON TRAINING DATA**
```{r, message = FALSE, warning = FALSE, collapse = TRUE, strip.white = TRUE}
lda_model <- lda(classe ~ ., data = training)

confusionMatrix(predict(lda_model)$class, training$classe)
```

**BUILD PREDICTION USING NAIVE BAYES ON TRAINING DATA**
```{r, message = FALSE, warning = FALSE, collapse = TRUE, strip.white = TRUE}
nb_model <- NaiveBayes(classe ~ ., data = training)

confusionMatrix(predict(nb_model, newdata = training)$class, training$classe)
```

**APPLYING RANDOM FOREST MODEL TO TESTING DATA**  
Based on the above results, random forest has the highest accuracy of the models tested (>99%). So we'll use look at the predictions of the random forest model on the test data as a further validation, and to get another estimate of "out-of-sample" accuracy/error.
```{r, message = FALSE, warning = FALSE, collapse = TRUE, strip.white = TRUE}
testing$ranfor_pred_classe <- predict(ranfor_model, newdata = testing)

oos_error <- 1 - with(testing, confusionMatrix(ranfor_pred_classe, classe))$overall[1]
```
The estimated out-of-sample error for this random forest model is `r paste0(round(oos_error*100, 2), "%")`.  
  
**SETTING UP FINAL TESTING DATA SET (20 OBSERVATIONS), THEN APPLYING RANDOM FOREST PREDICTION**
```{r, message = FALSE, warning = FALSE, collapse = TRUE, strip.white = TRUE}
#Take provided testing data set, limit down to same columns as training set was
finaltesting <- subset(pmltesting, select = colsToKeep)

finaltesting$ranfor_pred_classe <- predict(ranfor_model, newdata = finaltesting)

answers <- finaltesting$ranfor_pred_classe

answers
```

**WRITING PREDICTIONS ON PROVIDED FINAL TESTING SET TO INDIVIDUAL FILES**
```{r, message = FALSE, warning = FALSE, collapse = TRUE, eval = FALSE, strip.white = TRUE}
#Function provided by course to create 1 file per answer
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(answers)
```
The answer files generated by this function were the ones uploaded to the course website as part of the course project assignment.